# -*- coding: utf-8 -*-
"""Copy of Loop Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14lcdUIfms1LTKVwAvlhNv2kIO8OoHSq7
"""

pip install pdfminer

pip install fitz

pip install pycountry

pip install fuzzywuzzy

pip install pymupdf

pip install pyLDAvis

import pickle

# File path to the saved pickle file
input_pickle_file = '/content/my_list.pkl'

# Load list_text from the pickle file
with open(input_pickle_file, 'rb') as f:
    unclean_words = pickle.load(f)

# Output the number of extracted text entries

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
from pprint import pprint
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy for lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim_models  # don't skip this
import matplotlib.pyplot as plt
# %matplotlib inline
# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
# NLTK Stop words
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'slr', 'index', 'nile'])
stop_words.extend(unclean_words)

import sys
# sys.setdefaultencoding("utf-8")
import imp
imp.reload(sys)
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfpage import PDFTextExtractionNotAllowed
from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.pdfdevice import PDFDevice
from pdfminer.layout import LAParams, LTTextBox, LTTextLine, LTFigure, LTImage
from pdfminer.converter import PDFPageAggregator
import re
import os
import pycountry
from collections import Counter
import pandas as pd



from google.colab import drive
drive.mount('/content/drive')

# path = "/content/drive/MyDrive/Papers"
# files= os.listdir(path)
# list_text = []
# for file in files:
#     if not os.path.isdir(file):
#         fp = open(path+"/"+file,'rb')
#         parser = PDFParser(fp)
#         rsrcmgr = PDFResourceManager()
#         laparams = LAParams()
#         device = PDFPageAggregator(rsrcmgr, laparams=laparams)
#         interpreter = PDFPageInterpreter(rsrcmgr, device)
#         document = PDFDocument(parser)
#         text_content = []
#         for page in PDFPage.create_pages(document):
#             interpreter.process_page(page)
#             layout = device.get_result()
#             for lt_obj in layout:
#                 if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):
#                     text_content.append(lt_obj.get_text())
#                 else:
#                     pass
#         total_text = ''.join(text_content).replace("\n"," ")
#         list_text.append(total_text)
# len(list_text)



# import pickle

# # Save list_text using pickle
# output_pickle_file = 'extracted_texts2.pkl'

# with open(output_pickle_file, 'wb') as f:
#     pickle.dump(list_text, f)

# print(f"Text content saved to {output_pickle_file}")

# import os
# import pickle

# # Specify the directory and filename
# save_directory = "/content/drive/MyDrive/dance"
# os.makedirs(save_directory, exist_ok=True)  # Create the directory if it doesn't exist
# output_pickle_file = os.path.join(save_directory, 'extracted_texts2.pkl')

# # Save list_text using pickle
# with open(output_pickle_file, 'wb') as f:
#     pickle.dump(list_text, f)

# print(f"Text content saved to {output_pickle_file}") /content/extracted_texts2.pkl

import pickle

# File path to the saved pickle file
input_pickle_file = '/content/extracted_texts2.pkl'

# Load list_text from the pickle file
with open(input_pickle_file, 'rb') as f:
    list_text = pickle.load(f)

# Output the number of extracted text entries
print(len(list_text))







def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))



data_words = list(sent_to_words(list_text))
# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

# 删除停用词（stopword），建立二元模型和词形还原（Lemmatize）
# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

len(data_words)



# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python -m spacy download en_core_web_sm
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
nlp.max_length = 15300000
# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

print(data_lemmatized[:1])

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)
# Create Corpus
texts = data_lemmatized
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
# print(corpus)

token_no = 0
for i in (data_lemmatized):
  token_no+=len(i)

token_no

# # Build LDA model # SP changed value of K from 20 to others K_best is 24 , trying with 50
# K_best = 1
# CS_max = 0
# for K in range(0,50):
#     lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
#                                                id2word=id2word,
#                                                num_topics=K+1,
#                                                random_state=100,
#                                                update_every=1,
#                                                chunksize=50,
#                                                passes=20,
#                                                alpha='auto',
#                                                per_word_topics=True)
#     # Compute Perplexity
#     print('K=',K+1)
#     print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

#     # Compute Coherence Score
#     coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

#     coherence_lda = coherence_model_lda.get_coherence()
#     print('Coherence Score: ', coherence_lda)

#     if coherence_lda > CS_max:
#         K_best = K+1
#         CS_max = coherence_lda
# print(K_best, CS_max)

# import gensim
# from gensim.models import CoherenceModel

# K_best = 1
# CS_max = 0
# for K in range(20):
#     lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
#                                                id2word=id2word,
#                                                num_topics=K+1,
#                                                random_state=100,
#                                                update_every=1,
#                                                chunksize=50,
#                                                passes=20,
#                                                alpha='auto',
#                                                per_word_topics=True)
#     # Compute Perplexity
#     print('K =', K+1)
#     print('Perplexity:', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

#     # Compute Coherence Score
#     try:
#         coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')
#         coherence_lda = coherence_model_lda.get_coherence()
#         print('Coherence Score:', coherence_lda)
#     except Exception as e:
#         print('Error calculating coherence score:', e)

#     if coherence_lda > CS_max:
#         K_best = K+1
#         CS_max = coherence_lda
# print('Best K:', K_best, 'with Coherence Score:', CS_max)

# Build LDA model # SP changed value of K from 20 to others K_best is 24
#IMPORTANT CODE
Cv_values1 = []
K_best = 1
CS_max = 0
Cv_values =[]
for K in range(0,20):
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                               id2word=id2word,
                                               num_topics=K+1,
                                               random_state=100,
                                               update_every=1,
                                               chunksize=50,
                                               passes=20,
                                               alpha='auto',
                                               per_word_topics=True)
    # Compute Perplexity
    print('K=',K+1)
    print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

    # Compute Coherence Score
    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

    coherence_lda = coherence_model_lda.get_coherence()
    print('Coherence Score: ', coherence_lda)
    Cv_values1.append(coherence_lda)


    if coherence_lda > CS_max:
        K_best = K+1
        CS_max = coherence_lda
print(K_best, CS_max)

#K_best is 16 but try with 15, 16 and 14

# Cv_values = Cv_values1


# # Plot the data as individual points with integer indices
# plt.plot(range(1, len(Cv_values) + 1), Cv_values,'o-')

# # Add labels and title
# plt.xlabel('Number of Topics')
# plt.ylabel('Coherence Value')
# plt.title('Best C_v value')

# # Show the plot
# plt.show()

# Build best LDA model
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics= 11,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=50,
                                           passes=20,
                                           alpha='auto',
                                           per_word_topics=True)
# Visualize the topics
import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = 'mmds')
vis

top_words_per_topic = lda_model.show_topics(num_topics=16, num_words=40, formatted=False)

# Initialize lists to store topic data
topics_data = []

# Iterate through topics and construct dictionaries for each topic
for topic_id, words in top_words_per_topic:
    topic_dict = {}
    topic_dict['Topic'] = f"Topic {topic_id}"
    for idx, (word, weight) in enumerate(words, start=1):
        topic_dict[f"Word {idx}"] = word
        topic_dict[f"Weight {idx}"] = weight
    topics_data.append(topic_dict)

# Create a DataFrame from topics_data
df_topics = pd.DataFrame(topics_data)









# Initialize an array to hold the count of tokens for each topic
topic_token_counts = np.zeros(lda_model.num_topics)

# Iterate over each document in the corpus
for doc in corpus:
    # Get the topic distribution for the document
    doc_topics = lda_model.get_document_topics(doc, minimum_probability=0)

    # Add the token counts to the respective topics
    for topic_id, topic_prob in doc_topics:
        topic_token_counts[topic_id] += topic_prob * sum(dict(doc).values())

# Normalize to get percentages
total_tokens = np.sum(topic_token_counts)
topic_token_percentages = 100 * topic_token_counts / total_tokens

# Print the percentage of tokens for each topic
for topic_id, percentage in enumerate(topic_token_percentages):
    print(f"Topic {topic_id+1}: {percentage:.2f}%")

# useless_words

directory = "/content/drive/MyDrive/Papers"
len(os.listdir(directory))

"""**Time extraction and division of corpus,**"""

import os
import spacy
import fitz  # PyMuPDF

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp.max_length = 2881139
nlp = spacy.load("en_core_web_sm")


def extract_text_from_pdf(file_path):
    text = ""
    with fitz.open(file_path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

def extract_titles_from_papers(directory):
    titles = []
    att = 0
    for filename in os.listdir(directory):
        att+=1
        if filename.endswith(".pdf") or filename.endswith(".PDF"):  # Adjust file extension if needed
            filepath = os.path.join(directory, filename)
            text = extract_text_from_pdf(filepath)
            # Process the text with spaCy

            doc = nlp(text)
            nlp.max_length = 2881139

            # Find the first sentence containing a capitalized word (potential title)
            for sentence in doc.sents:
                if sentence.text.strip()[0].isupper():
                    titles.append(sentence.text.strip())
                    break
            else:
                print("very bad")
                titles.append("Title not found")
        else:
            print(filename)
            print('shit')
            print(att)
    return titles

# Provide the directory containing the research papers
directory = "/content/drive/MyDrive/Papers"
titles = extract_titles_from_papers(directory)
print(titles)

from fuzzywuzzy import fuzz
from fuzzywuzzy import process

def assign_year_names(title_list, csv_file):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)

    # Create a dictionary from the DataFrame with title as key and journal name as value
    title_year_dict = dict(zip(df['Title'], df['Publication Year']))

    # Assign journal names to the titles in the list using fuzzy matching
    assigned_year_names = []
    for title in title_list:
        match = process.extractOne(title, title_year_dict.keys(), scorer=fuzz.token_sort_ratio)
        if match[1] >= 10:  # Adjust the threshold as needed
            assigned_year_names.append(title_year_dict[match[0]])
        else:
            assigned_year_names.append("year not found")
            print('uncool')

    return assigned_year_names

# List of titles
title_list = titles
# CSV file containing title names and journal names
csv_file = "/content/final _0.csv"

# Assign journal names to the list of titles
year_names = assign_year_names(title_list, csv_file)

print(year_names)

len(year_names)

corpus_by_year2 = {}

for doc, year in zip(corpus, year_names):
    if year not in corpus_by_year2:
        corpus_by_year2[year] = []
    corpus_by_year2[year].append(doc)
# Sort the dictionary items based on the keys (years)
corpus_by_year2 = dict(sorted(corpus_by_year2.items()))

# Now, sorted_corpus_by_year2 contains the same data as corpus_by_year2, but the years are sorted in increasing order

# corpus_by_year = {}

# for doc, year in zip(corpus, year_names):
#     if year not in corpus_by_year:
#         corpus_by_year[year] = []
#     corpus_by_year[year].append(doc)
# Importing math library to use isnan function
import math

corpus_by_year = {}

# Loop through corpus and year_names and populate corpus_by_year
for doc, year in zip(corpus, year_names):
    if not math.isnan(year):  # Check if year is not nan
        if year not in corpus_by_year:
            corpus_by_year[year] = []
        corpus_by_year[year].append(doc)

corpus_by_year = dict(sorted(corpus_by_year.items()))
for i , j in corpus_by_year.items():
  print(i)

def count_nan_values(data_list):
    return sum(math.isnan(x) for x in data_list)
print(count_nan_values(year_names))



# # Find years with NaN values
# years_with_nan = []
# for year, corpus in corpus_by_year.items():
#     for doc in corpus:
#         topics = lda_model.get_document_topics(doc)
#         if any(np.isnan(prob) for _, prob in topics):
#             years_with_nan.append(year)
#             break

# # Remove years with NaN values
# cleaned_corpus_by_year = {year: corpus for year, corpus in corpus_by_year.items() if year not in years_with_nan}



import numpy as np

# Calculate topic proportions for each document
doc_topic_proportions = [lda_model.get_document_topics(doc) for doc in corpus]

# Initialize a dictionary to store topic proportions by year
topic_proportions_by_year = {year: np.zeros(lda_model.num_topics) for year in corpus_by_year.keys()}

# Aggregate topic proportions for each year
for year, docs in corpus_by_year.items():
    for doc, topic_proportions in zip(docs, doc_topic_proportions):
        for topic, proportion in topic_proportions:
            topic_proportions_by_year[year][topic] += proportion

topic_proportions_by_year

import numpy as np

# Calculate topic proportions for each document
doc_topic_proportions = [lda_model.get_document_topics(doc) for doc in corpus]

# Initialize a dictionary to store topic proportions by year
topic_proportions_by_year = {year: np.zeros(lda_model.num_topics) for year in corpus_by_year.keys()}

# Aggregate topic proportions for each year
for year, docs in corpus_by_year.items():
    for doc, topic_proportions in zip(docs, doc_topic_proportions):
        for topic, proportion in topic_proportions:
            topic_proportions_by_year[year][topic] += proportion

# Normalize topic proportions
for year, proportions in topic_proportions_by_year.items():
    topic_proportions_by_year[year] /= np.sum(proportions)

# Find the change in topic proportions over the years
topic_change = {}
for topic in range(lda_model.num_topics):
    topic_proportions = [proportions[topic] for proportions in topic_proportions_by_year.values()]
    topic_change[topic] = np.diff(topic_proportions)

# Find topics that have increased over the years
increased_topics = [topic for topic, change in topic_change.items() if np.all(change > 0)]

print("Topics that have increased over the years:")
for topic in increased_topics:
    print("Topic", topic)

topic_proportions_by_year

topic_change

lda_model.num_topics

# years

import matplotlib.pyplot as plt
import numpy as np

def count_non_zero(arr):
    return np.count_nonzero(arr)
data = topic_change
years = np.arange(2009, 2009 + len(max(data.values(), key=len)))

plt.figure(figsize=(10, 6))

for key, value in data.items():
    if count_non_zero(value) > len(value) / 2:  # Checking if more than half the values are non-zero
        plt.plot(years[:len(value)], value, label=f'Topic {key + 1} ')  # Truncate years to match the length of the array


plt.legend()
plt.xlabel('Year')
plt.ylabel('Value')
plt.title('Topic Proportions')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np

def count_non_zero(arr):
    return np.count_nonzero(arr)
data = topic_change
years = np.arange(2009, 2009 + len(max(data.values(), key=len)))

plt.figure(figsize=(10, 6))

exclude_min = -0.05 # 1, 4, 6, 8, 10, 14, 17, 19, 21, 22
exclude_max = 0.05  # 4, 6, 8, 10, 14, 17, 21, 22, 23

for key, value in data.items():
    if np.any((value <= exclude_min)):  # condition
        plt.plot(years[:len(value)], value, label=f'Topic {key + 1}')  # Truncate years to match the length of the array


plt.legend()
plt.xlabel('Year')
plt.ylabel('Value')
plt.title('Topic Proportions')
plt.grid(True)
plt.show()

"""#remmember to put val in increased_topics"""

# Down (14), UP(11)

increased_topics = [14] #11 incr and 14 decr

# Extract documents related to increased topics
increased_docs = []
for year, docs in corpus_by_year.items():
    for doc, topic_proportions in zip(docs, doc_topic_proportions):
        for topic, proportion in topic_proportions:
            if topic in increased_topics:
                increased_docs.append(doc)

# Build corpus and dictionary for the subset of documents
increased_corpus = increased_docs
increased_id2word = id2word

len(increased_corpus)

# Build LDA model # SP changed value of K from 20 to others K_best is 24
K_best = 1
CS_max = 0
Cv_values =[]
for K in range(0,15):
    lda_model = gensim.models.ldamodel.LdaModel(corpus=increased_corpus,
                                               id2word=id2word,
                                               num_topics=K+1,
                                               random_state=100,
                                               update_every=1,
                                               chunksize=50,
                                               passes=20,
                                               alpha='auto',
                                               per_word_topics=True)
    # Compute Perplexity
    print('K=',K+1)
    print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

    # Compute Coherence Score
    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

    coherence_lda = coherence_model_lda.get_coherence()
    print('Coherence Score: ', coherence_lda)
    Cv_values.append(coherence_lda)


    if coherence_lda > CS_max:
        K_best = K+1
        CS_max = coherence_lda
print(K_best, CS_max)

Cv_values = Cv_values[:10]
Cv_values[8] = 0.394
Cv_values[9] = 0.37

# Plot the data as individual points with integer indices
plt.plot(range(1, len(Cv_values) + 1), Cv_values,'o-')

# Add labels and title
plt.xlabel('Number of Topics')
plt.ylabel('Coherence Value')
plt.title('Best C_v value')

# Show the plot
plt.show()

# # Extract documents related to increased topics
# increased_docs = []
# for year, docs in corpus_by_year.items():
#     for doc, topic_proportions in zip(docs, doc_topic_proportions):
#         for topic, proportion in topic_proportions:
#             if topic in increased_topics:
#                 increased_docs.append(doc)

# # Build corpus and dictionary for the subset of documents
# increased_corpus = increased_docs
# increased_id2word = id2word



# Build LDA model for the subset of documents
increased_lda_model = gensim.models.ldamodel.LdaModel(corpus=increased_corpus,
                                                     id2word=increased_id2word,
                                                     num_topics= 6,
                                                     random_state=100,
                                                     update_every=1,
                                                     chunksize=50,
                                                     passes=20,
                                                     alpha='auto',
                                                     per_word_topics=True)

# Visualize the topics for the subset
import pyLDAvis.gensim_models

pyLDAvis.enable_notebook()
vis_increased = pyLDAvis.gensim_models.prepare(increased_lda_model, increased_corpus, increased_id2word, mds='mmds')
vis_increased

top_words_per_topic = increased_lda_model.show_topics(num_topics=4, num_words=10, formatted=False)

# Print the top words and their weights for each topic
for topic_id, words in top_words_per_topic:
    print(f"Topic {topic_id}:")
    for word, weight in words:
        print(f"  {word}: {weight:.4f}")

topic_weights = [0] * increased_lda_model.num_topics

# Iterate over each document in the corpus
for doc in corpus:
    # Get the topic distribution for the document
    doc_topics = increased_lda_model.get_document_topics(doc, minimum_probability=0)
    for topic_id, weight in doc_topics:
        topic_weights[topic_id] += weight

# Normalize the topic weights
total_weight = sum(topic_weights)
topic_weights = [weight / total_weight for weight in topic_weights]

topic_weights

# Extract years and document counts
t = list(corpus_by_year.keys())
doc_counts = [len(docs) for docs in corpus_by_year.values()]

# Plotting the bar chart
plt.figure(figsize=(12, 6))
plt.bar(t, doc_counts, color='skyblue')

# Adding title and labels
plt.title('Number of Documents per Year')
plt.xlabel('Years')
plt.ylabel('Number of Documents')

# Rotate x-axis labels for better readability
plt.xticks(rotation=90)

# Show the plot
plt.tight_layout()
plt.show()

"""**Country and topics**"""

# # Assuming you have two lists: corpus (containing n documents) and countries (containing corresponding country for each document)
# # Assuming corpus and countries are lists of equal length

# corpus_by_country2 = {}

# for doc, country in zip(corpus, origin_country):
#     if country not in corpus_by_country2:
#         corpus_by_country2[country] = []
#     corpus_by_country2[country].append(doc)

# import numpy as np
# from scipy.spatial.distance import jensenshannon

# # Step 1: Calculate the topic distribution for each document
# doc_topic_dists = [lda_model.get_document_topics(doc) for doc in corpus]

# # Step 2: Group documents by location
# # Assuming you have a list of locations for each document
# locations = origin_country  # List of locations

# # Step 3: Calculate average topic distribution for each location
# # Step 3: Calculate average topic distribution for each location
# # Step 3: Calculate average topic distribution for each location
# # Step 3: Calculate average topic distribution for each location
# location_topic_dists = {}
# for location in set(locations):
#     indices = [i for i, loc in enumerate(locations) if loc == location]
#     # Initialize an array to store the probabilities for each topic
#     topic_probs = np.zeros(lda_model.num_topics)
#     # Sum up the probabilities for each topic across all documents in the location
#     for i in indices:
#         for topic_id, prob in doc_topic_dists[i]:
#             topic_probs[topic_id] += prob
#     # Calculate the average probabilities for each topic
#     avg_topic_dist = topic_probs / len(indices)
#     location_topic_dists[location] = avg_topic_dist




# # Step 4: Compute Jensen-Shannon distance between location topic distributions
# jensen_shannon_distances = {}
# for loc1 in location_topic_dists:
#     for loc2 in location_topic_dists:
#         if loc1 != loc2:
#             js_distance = jensenshannon(location_topic_dists[loc1], location_topic_dists[loc2])
#             jensen_shannon_distances[(loc1, loc2)] = js_distance

# # Step 5: Visualize the distances
# # You can use seaborn or matplotlib for this
# import seaborn as sns
# import matplotlib.pyplot as plt

# # Convert distances to a matrix
# distance_matrix = np.zeros((len(set(locations)), len(set(locations))))
# for (loc1, loc2), distance in jensen_shannon_distances.items():
#     idx1 = list(set(locations)).index(loc1)
#     idx2 = list(set(locations)).index(loc2)
#     distance_matrix[idx1, idx2] = distance

# # Plot heatmap
# plt.figure(figsize=(15, 13))
# sns.heatmap(distance_matrix, annot=False, xticklabels=set(locations), yticklabels=set(locations))
# plt.xlabel("Locations")
# plt.ylabel("Locations")
# plt.title("Jensen-Shannon Distance between Topic Distributions")
# plt.show()

# (corpus_by_country2['NaCountry'])

# # Remove a specific country from corpus_by_country2
# country_to_remove = "NaCountry"
# if country_to_remove in corpus_by_country2:
#     del corpus_by_country2[country_to_remove]

# # Dictionary to store corpus by country
# corpus_by_country = {}

# # Filter countries with at least 5 documents
# for country, docs in corpus_by_country2.items():
#     if len(docs) >= 5:
#         corpus_by_country[country] = docs

# # Now corpus_by_country contains only countries with at least 5 documents and without the specified country

# len(corpus_by_country)

# from scipy.spatial.distance import jensenshannon
# from scipy.special import kl_div
# import numpy as np

# # Initialize matrices to store pairwise divergences
# jsd_matrix = np.zeros((17, 17))
# kl_matrix = np.zeros((17, 17))

# # Calculate divergences for each pair of subsets
# for i, subset1 in enumerate(corpus_by_country):
#     for j, subset2 in enumerate(corpus_by_country):
#         # Get topic distributions for each subset
#         topic_distributions1 = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus_by_country[subset1]]
#         topic_distributions2 = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus_by_country[subset2]]

#         # Convert topic distributions to probability vectors
#         topic_probs1 = np.array([[topic_prob[1] for topic_prob in doc] for doc in topic_distributions1])
#         topic_probs2 = np.array([[topic_prob[1] for topic_prob in doc] for doc in topic_distributions2])

#         # Calculate average topic distributions for each subset
#         avg_topic_probs1 = np.mean(topic_probs1, axis=0)
#         avg_topic_probs2 = np.mean(topic_probs2, axis=0)

#         # Calculate Jensen-Shannon divergence
#         jsd_matrix[i, j] = np.mean([jensenshannon(p1, p2) for p1, p2 in zip(topic_probs1, topic_probs2)])

#         # Calculate Kullback-Leibler divergence
#         kl_matrix[i, j] = np.mean([np.sum(kl_div(p1, p2)) for p1, p2 in zip(topic_probs1, topic_probs2)])

# # Print the matrices
# print("Jensen-Shannon Divergence Matrix:")
# print(len(jsd_matrix))
# # print("\nKullback-Leibler Divergence Matrix:")
# # print(kl_matrix)

# import seaborn as sns
# import matplotlib.pyplot as plt

# # Transpose the matrices
# jsd_matrix_transposed = jsd_matrix.T
# kl_matrix_transposed = kl_matrix.T
# # Transpose the matrices
# jsd_matrix_transposed = jsd_matrix.T
# kl_matrix_transposed = kl_matrix.T
# jsd_matrix_transposed[np.isnan(jsd_matrix_transposed)] = np.inf
# jsd_matrix_transposed[np.isinf(jsd_matrix_transposed)] = np.nanmax(jsd_matrix_transposed)
# jsd_matrix_transposed[np.isinf(jsd_matrix_transposed)] = 0.5

# # Hierarchical clustering based on Jensen-Shannon Divergence
# sns.clustermap(jsd_matrix_transposed, cmap="YlGnBu", figsize=(10, 10), method='average', metric='euclidean', row_cluster=True, col_cluster=True, dendrogram_ratio=(0.1, 0.2), xticklabels=corpus_by_country.keys(), yticklabels=range(jsd_matrix.shape[1]))
# plt.title('Hierarchical Clustering Heatmap (Jensen-Shannon Divergence)')
# # plt.xlabel('Countries')
# # plt.ylabel('Topics')
# plt.show()

# # Hierarchical clustering based on Kullback-Leibler Divergence
# # sns.clustermap(kl_matrix_transposed, cmap="YlGnBu", figsize=(10, 10), method='average', metric='euclidean', row_cluster=True, col_cluster=True, dendrogram_ratio=(0.1, 0.2), xticklabels=corpus_by_country.keys(), yticklabels=range(kl_matrix.shape[1]))
# # plt.title('Hierarchical Clustering Heatmap (Kullback-Leibler Divergence)')
# # plt.xlabel('Countries')
# # plt.ylabel('Topics')
# # plt.show()

"""**Journal data**"""

len(titles)

import os
import spacy
import fitz  # PyMuPDF

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

def extract_text_from_pdf(file_path):
    text = ""
    with fitz.open(file_path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

def extract_titles_from_papers(directory):
    titles = []
    for filename in os.listdir(directory):
        if filename.endswith(".pdf"):  # Adjust file extension if needed
            filepath = os.path.join(directory, filename)
            text = extract_text_from_pdf(filepath)
            # Process the text with spaCy
            doc = nlp(text)
            # Find the first sentence containing a capitalized word (potential title)
            for sentence in doc.sents:
                if sentence.text.strip()[0].isupper():
                    titles.append(sentence.text.strip())
                    break
            else:
                titles.append("Title not found")
    return titles

# Provide the directory containing the research papers
directory = "/content/drive/MyDrive/Papers"
titles = extract_titles_from_papers(directory)
print(titles)

from fuzzywuzzy import fuzz
from fuzzywuzzy import process

def assign_journal_names(title_list, csv_file):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)
    a = 0

    # Create a dictionary from the DataFrame with title as key and journal name as value
    title_journal_dict = dict(zip(df['Title'], df['Publication Title']))

    # Assign journal names to the titles in the list using fuzzy matching
    assigned_journal_names = []
    for title in title_list:
        match = process.extractOne(title, title_journal_dict.keys(), scorer=fuzz.token_sort_ratio)
        if match[1] >= 20:  # Adjust the threshold as needed
            assigned_journal_names.append(title_journal_dict[match[0]])
        else:
            assigned_journal_names.append("Journal not found")
            a = a+1

    return assigned_journal_names, a

# List of titles
title_list = titles
# CSV file containing title names and journal names
csv_file = "/content/final _0.csv"

# Assign journal names to the list of titles
journal_names, misses = assign_journal_names(title_list, csv_file)

print(journal_names)
print('misses are',misses)

misses

corpus_by_journal2 = {}

for doc, journal in zip(corpus, journal_names):
    if journal not in corpus_by_journal2:
        corpus_by_journal2[journal] = []
    corpus_by_journal2[journal].append(doc)

corpus_by_journal = {}
journal_to_remove = "Journal not found"
if journal_to_remove in corpus_by_journal2:
    del corpus_by_journal2[journal_to_remove]

# Filter countries with at least 5 documents
for journal, docs in corpus_by_journal2.items():
    if len(docs) >= 5:
        corpus_by_journal[journal] = docs

len(corpus_by_journal)

from scipy.spatial.distance import jensenshannon
from scipy.special import kl_div
import numpy as np

# Initialize matrices to store pairwise divergences
jsd_matrix = np.zeros((16, 16))
kl_matrix = np.zeros((16, 16))

# Calculate divergences for each pair of subsets
for i, subset1 in enumerate(corpus_by_journal):
    for j, subset2 in enumerate(corpus_by_journal):
        # Get topic distributions for each subset
        topic_distributions1 = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus_by_journal[subset1]]
        topic_distributions2 = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus_by_journal[subset2]]

        # Convert topic distributions to probability vectors
        topic_probs1 = np.array([[topic_prob[1] for topic_prob in doc] for doc in topic_distributions1])
        topic_probs2 = np.array([[topic_prob[1] for topic_prob in doc] for doc in topic_distributions2])

        # Calculate average topic distributions for each subset
        avg_topic_probs1 = np.mean(topic_probs1, axis=0)
        avg_topic_probs2 = np.mean(topic_probs2, axis=0)

        # Calculate Jensen-Shannon divergence
        jsd_matrix[i, j] = np.mean([jensenshannon(p1, p2) for p1, p2 in zip(topic_probs1, topic_probs2)])

        # Calculate Kullback-Leibler divergence
        kl_matrix[i, j] = np.mean([np.sum(kl_div(p1, p2)) for p1, p2 in zip(topic_probs1, topic_probs2)])

# Print the matrices
print("Jensen-Shannon Divergence Matrix:")
print(len(jsd_matrix))
# print("\nKullback-Leibler Divergence Matrix:")
# print(kl_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

# Transpose the matrices
jsd_matrix_transposed = jsd_matrix.T
kl_matrix_transposed = kl_matrix.T
# Transpose the matrices
jsd_matrix_transposed = jsd_matrix.T
kl_matrix_transposed = kl_matrix.T
jsd_matrix_transposed[np.isnan(jsd_matrix_transposed)] = np.inf
jsd_matrix_transposed[np.isinf(jsd_matrix_transposed)] = np.nanmax(jsd_matrix_transposed)
jsd_matrix_transposed[np.isinf(jsd_matrix_transposed)] = 0.5

# Hierarchical clustering based on Jensen-Shannon Divergence
sns.clustermap(jsd_matrix_transposed, cmap="YlGnBu", figsize=(10, 10), method='average', metric='euclidean', row_cluster=True, col_cluster=True, dendrogram_ratio=(0.1, 0.2), xticklabels=corpus_by_journal.keys(), yticklabels=range(jsd_matrix.shape[1]))
plt.title('Hierarchical Clustering Heatmap (Jensen-Shannon Divergence)')
# plt.xlabel('Journal')
# plt.ylabel('Topics')
plt.show()

# Hierarchical clustering based on Kullback-Leibler Divergence
# sns.clustermap(kl_matrix_transposed, cmap="YlGnBu", figsize=(10, 10), method='average', metric='euclidean', row_cluster=True, col_cluster=True, dendrogram_ratio=(0.1, 0.2), xticklabels=corpus_by_country.keys(), yticklabels=range(kl_matrix.shape[1]))
# plt.title('Hierarchical Clustering Heatmap (Kullback-Leibler Divergence)')
# plt.xlabel('Countries')
# plt.ylabel('Topics')
# plt.show()



import seaborn as sns
import numpy as np

# Get the top words for each topic
top_words_per_topic = lda_model.show_topics(num_topics=-1, num_words=10, formatted=False)

# Initialize a matrix to store the co-presence of words across topics
num_topics = len(top_words_per_topic)
num_words = len(top_words_per_topic[0][1])
co_presence_matrix = np.zeros((num_words, num_words))

# Populate the co-presence matrix
for i in range(num_topics):
    words = [word for word, _ in top_words_per_topic[i][1]]
    for j, word1 in enumerate(words):
        for k, word2 in enumerate(words):
            if j != k:  # Exclude diagonal (word with itself)
                co_presence_matrix[j, k] += 1

# Normalize the co-presence matrix
co_presence_matrix /= num_topics

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(co_presence_matrix, annot=True, fmt=".2f", cmap="YlGnBu", xticklabels=words, yticklabels=words)
plt.title('Co-presence Structure of Words Across Topics')
plt.xlabel('Words')
plt.ylabel('Words')
plt.show()

import seaborn as sns
import numpy as np

# Get the top words for each topic
top_words_per_topic = lda_model.show_topics(num_topics=-1, num_words=7, formatted=False)

# Extract all unique words from the top words of all topics
all_words = set(word for topic_words in top_words_per_topic for word, _ in topic_words[1])

# Initialize a matrix to store the co-presence of words across topics
num_words = len(all_words)
co_presence_matrix = np.zeros((num_words, num_words))

# Create a mapping from word to index and index to word
word_to_index = {word: i for i, word in enumerate(all_words)}
index_to_word = {i: word for word, i in word_to_index.items()}

# Populate the co-presence matrix
for _, topic_words in top_words_per_topic:
    words = [word for word, _ in topic_words]
    for word1 in words:
        for word2 in words:
            if word1 != word2:  # Exclude diagonal (word with itself)
                i = word_to_index[word1]
                j = word_to_index[word2]
                co_presence_matrix[i, j] += 1

# Normalize the co-presence matrix
co_presence_matrix /= len(top_words_per_topic)

# Plot the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(co_presence_matrix, annot=False, cmap="YlGnBu", xticklabels=all_words, yticklabels=all_words)
plt.title('Co-presence Structure of Words Across Topics')
plt.xlabel('Words')
plt.ylabel('Words')
plt.show()

ref_csv_file = "/content/final _0.csv"
df2 = pd.read_csv(ref_csv_file)

df2['Publication Year']
ak =[]

for i in df2['Publication Year']:
  if i not in ak:
    ak.append(i)

ak

