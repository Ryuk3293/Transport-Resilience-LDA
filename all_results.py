# -*- coding: utf-8 -*-
"""All Results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JmqTzcmUIjCnGAC1K2Nq4c-MHMN7U_f9
"""

pip install pdfminer

pip install fitz

pip install pyLDAvis

pip install fuzzywuzzy

pip install pymupdf

pip install pycountry

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
from pprint import pprint
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy for lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim_models  # don't skip this
import matplotlib.pyplot as plt
# %matplotlib inline
# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
# NLTK Stop words
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'slr', 'index', 'nile'])

import sys
# sys.setdefaultencoding("utf-8")
import imp
imp.reload(sys)
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfpage import PDFTextExtractionNotAllowed
from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.pdfdevice import PDFDevice
from pdfminer.layout import LAParams, LTTextBox, LTTextLine, LTFigure, LTImage
from pdfminer.converter import PDFPageAggregator
import re
import os
import pycountry
from collections import Counter

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/Papers" #文件夹目录
files= os.listdir(path)
list_text = []
for file in files:
    if not os.path.isdir(file):
        fp = open(path+"/"+file,'rb')
        parser = PDFParser(fp)
        rsrcmgr = PDFResourceManager()
        laparams = LAParams()
        device = PDFPageAggregator(rsrcmgr, laparams=laparams)
        interpreter = PDFPageInterpreter(rsrcmgr, device)
        document = PDFDocument(parser)
        text_content = []
        for page in PDFPage.create_pages(document):
            interpreter.process_page(page)
            layout = device.get_result()
            for lt_obj in layout:
                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):
                    text_content.append(lt_obj.get_text())
                else:
                    pass
        total_text = ''.join(text_content).replace("\n"," ")
        list_text.append(total_text) # 这里可以不停加pdf进入列表
len(list_text)



import os
import spacy
import fitz  # PyMuPDF

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

def extract_text_from_pdf(file_path):
    text = ""
    with fitz.open(file_path) as pdf:
        for page in pdf:
            text += page.get_text()
    return text

def extract_titles_from_papers(directory):
    titles = []
    for filename in os.listdir(directory):
        if filename.endswith(".pdf"):  # Adjust file extension if needed
            filepath = os.path.join(directory, filename)
            text = extract_text_from_pdf(filepath)
            # Process the text with spaCy
            doc = nlp(text)
            # Find the first sentence containing a capitalized word (potential title)
            for sentence in doc.sents:
                if sentence.text.strip()[0].isupper():
                    titles.append(sentence.text.strip())
                    break
            else:
                titles.append("Title not found")
    return titles

# Provide the directory containing the research papers
directory = "/content/drive/MyDrive/Papers"
titles = extract_titles_from_papers(directory)
print(titles)

len(list_text)

# # Load spaCy English model
# origin_country = []
# nlp = spacy.load("en_core_web_sm")

# # Process text and extract entities
# def extract_entities(text):
#     doc = nlp(text)
#     countries = [ent.text for ent in doc.ents if ent.label_ == "GPE" and ent.text in countries_set]  # Filter for geopolitical entities
#     country_freq = Counter(countries)
#     return countries, country_freq




# # Extract entities from the text
# for i in list_text:
#   countries, freq = extract_entities(i)
#   if not freq:
#       print("No country found in the text.")
#       origin_country.append('NA')
#   else:
#       # Get countries with the highest frequencies
#       most_common_countries = freq.most_common(1)  # Change 3 to the desired number of countries to retrieve
#       print("Countries with the highest frequencies:", most_common_countries)
#       origin_country.append(str(most_common_countries[0][0]))

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

# 删除停用词（stopword），建立二元模型和词形还原（Lemmatize）
# Define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

data_words = list(sent_to_words(list_text))
# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python -m spacy download en_core_web_sm
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

print(data_lemmatized[:1])

# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)
# Create Corpus
texts = data_lemmatized
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
# print(corpus)

len(corpus)

# Build LDA model # SP changed value of K from 20 to others, at range 25 k_best is 24.
K_best = 1
CS_max = 0
for K in range(0,50):
    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                               id2word=id2word,
                                               num_topics=K+1,
                                               random_state=100,
                                               update_every=1,
                                               chunksize=50,
                                               passes=20,
                                               alpha='auto',
                                               per_word_topics=True)
    # Compute Perplexity
    print('K=',K+1)
    print('Perplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.

    # Compute Coherence Score
    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

    coherence_lda = coherence_model_lda.get_coherence()
    print('Coherence Score: ', coherence_lda)

    if coherence_lda > CS_max:
        K_best = K+1
        CS_max = coherence_lda
print(K_best, CS_max)

# Build best LDA model, first k_best was 24, at range 50, it is 36
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics= 24,  #K_best,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=50,
                                           passes=20,
                                           alpha='auto',
                                           per_word_topics=True)
# Visualize the topics
import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = 'mmds')
vis

# import pyLDAvis.gensim_models

# # Get topic distribution for each year
# topic_distributions_by_year = {}
# for year, year_corpus in corpus_by_year.items():
#     topic_distributions_by_year[year] = [lda_model.get_document_topics(doc) for doc in year_corpus]

# # Prepare topic proportions for visualization
# topic_proportions_by_year = {}
# for year, topic_distributions in topic_distributions_by_year.items():
#     topic_proportions = [dict(topic_dist) for topic_dist in topic_distributions]
#     topic_proportions_by_year[year] = topic_proportions

# # Prepare pyLDAvis data
# vis_data = pyLDAvis.prepare(lda_model, topic_proportions_by_year.values(), topic_proportions_by_year.keys())

# # Display the visualization
# pyLDAvis.display(vis_data)

# import numpy as np
# from scipy.spatial.distance import jensenshannon

# # Step 1: Calculate the topic distribution for each document
# doc_topic_dists = [lda_model.get_document_topics(doc) for doc in corpus]

# # Step 2: Group documents by location
# # Assuming you have a list of locations for each document
# locations = origin_country  # List of locations

# # Step 3: Calculate average topic distribution for each location
# # Step 3: Calculate average topic distribution for each location
# # Step 3: Calculate average topic distribution for each location
# # Step 3: Calculate average topic distribution for each location
# location_topic_dists = {}
# for location in set(locations):
#     indices = [i for i, loc in enumerate(locations) if loc == location]
#     # Initialize an array to store the probabilities for each topic
#     topic_probs = np.zeros(lda_model.num_topics)
#     # Sum up the probabilities for each topic across all documents in the location
#     for i in indices:
#         for topic_id, prob in doc_topic_dists[i]:
#             topic_probs[topic_id] += prob
#     # Calculate the average probabilities for each topic
#     avg_topic_dist = topic_probs / len(indices)
#     location_topic_dists[location] = avg_topic_dist




# # Step 4: Compute Jensen-Shannon distance between location topic distributions
# jensen_shannon_distances = {}
# for loc1 in location_topic_dists:
#     for loc2 in location_topic_dists:
#         if loc1 != loc2:
#             js_distance = jensenshannon(location_topic_dists[loc1], location_topic_dists[loc2])
#             jensen_shannon_distances[(loc1, loc2)] = js_distance

# # Step 5: Visualize the distances
# # You can use seaborn or matplotlib for this
# import seaborn as sns
# import matplotlib.pyplot as plt

# # Convert distances to a matrix
# distance_matrix = np.zeros((len(set(locations)), len(set(locations))))
# for (loc1, loc2), distance in jensen_shannon_distances.items():
#     idx1 = list(set(locations)).index(loc1)
#     idx2 = list(set(locations)).index(loc2)
#     distance_matrix[idx1, idx2] = distance

# # Plot heatmap
# plt.figure(figsize=(15, 13))
# sns.heatmap(distance_matrix, annot=True, xticklabels=set(locations), yticklabels=set(locations))
# plt.xlabel("Locations")
# plt.ylabel("Locations")
# plt.title("Jensen-Shannon Distance between Topic Distributions")
# plt.show()

# # Assuming lda_model is your trained LDA model
# # Assuming corpus is your document-term matrix
# # Assuming country_labels is a list/array containing the country labels for each document

# from collections import defaultdict

# # Initialize a defaultdict to store topic distributions by country
# topic_distributions = defaultdict(list)

# # Iterate through each document in the corpus
# for i, doc in enumerate(corpus):
#     # Get the topic distribution for the document
#     doc_topics, _ = lda_model.get_document_topics(doc, per_word_topics=True)

#     # Extract the country label for the document
#     country = origin_country[i]  # Assuming country_labels is a list/array

#     # Append the topic distribution to the corresponding country
#     topic_distributions[country].append(doc_topics)

# # Convert defaultdict to numpy array
# for country in topic_distributions:
#     topic_distributions[country] = np.array(topic_distributions[country])

# # Now topic_distributions is a dictionary where each key is a country and the value is a numpy array
# # Each row in the numpy array represents the topic distribution for a document from that country



len(corpus)

from gensim.models.ldaseqmodel import LdaSeqModel
from gensim.corpora import Dictionary

# # Assuming 'texts_by_time' is a list of tokenized texts for each time slice
# dictionary = Dictionary(texts_by_time[0])  # Use the first time slice to create the dictionary
# corpus = [dictionary.doc2bow(texts) for texts in texts_by_time]

# Train LDASEQModel
ldaseq_model = LdaSeqModel(corpus, id2word=id2word, num_topics=12, time_slice=[5,6,7,11])

# Visualize topics over time
topics_over_time = ldaseq_model.print_topics(time=0)  # Get topics for the first time slice
print(topics_over_time)



# temporal DTM ??

import numpy as np
import matplotlib.pyplot as plt

# Initialize an empty list to store topic distributions over time
topic_distributions = []

# Iterate over each time slice to get the top words associated with each topic
for t in range(len(ldaseq_model.time_slice)):
    topics_at_time = ldaseq_model.print_topics(time=t)
    topic_distributions.append([topic[1] for topic in topics_at_time])

# Convert the list of topic distributions into a numpy array
topic_distributions_array = np.array(topic_distributions)

import numpy as np
import matplotlib.pyplot as plt

# Sample topic distributions array (replace this with your actual data)
# topic_distributions_array =topic_distributions # Example array with dimensions (time_slices, num_topics, num_top_words)

# Plot stacked area chart
plt.figure(figsize=(10, 6))
for i in range(topic_distributions_array.shape[1]):  # Iterate over topics
    plt.stackplot(range(topic_distributions_array.shape[0]), topic_distributions_array[:, i, :].T, labels=[f"Topic {i+1}"])
plt.xlabel("Time")
plt.ylabel("Word Distribution")
plt.title("Topic Distributions Over Time")
plt.legend(loc="upper left")
plt.show()

# import numpy as np
# import matplotlib.pyplot as plt

# # Replace this with your topic prevalence over time data
# topic_prevalence_over_time = ((topic_distributions)) # Example data, shape: (num_topics, num_time_slices)

# # Create a heatmap
# plt.figure(figsize=(10, 6))
# plt.imshow(topic_prevalence_over_time, cmap='viridis', aspect='auto')
# plt.colorbar(label='Topic Prevalence')
# plt.xlabel('Time Slice')
# plt.ylabel('Topic')
# plt.title('Topic Evolution Over Time')
# plt.show()

type(topic_distributions[0])

import numpy as np
import matplotlib.pyplot as plt

# Replace this with your topic prevalence over time data
topic_prevalence_over_time = np.random.rand(12, 4)  # Example data, shape: (num_topics, num_time_slices)

# Create a heatmap
plt.figure(figsize=(10, 6))
plt.imshow(topic_prevalence_over_time, cmap='viridis', aspect='auto')
plt.colorbar(label='Topic Prevalence')
plt.xlabel('Time Slice')
plt.ylabel('Topic')
plt.title('Topic Evolution Over Time')
plt.show()

np.random.rand(12, 4)

topic_distributions

import numpy as np
import matplotlib.pyplot as plt

# Data
data = topic_distributions

# Convert data to numpy array
data_array = np.zeros((len(data), len(data[0])), dtype=float)
for i, time_slice in enumerate(data):
    for j, (topic, prevalence) in enumerate(time_slice):
        data_array[i, j] = prevalence

# Create a heatmap
plt.figure(figsize=(10, 6))
plt.imshow(data_array, cmap='viridis', aspect='auto')
plt.colorbar(label='Topic Prevalence')
plt.xlabel('Time Slice')
plt.ylabel('Topic')
plt.title('Topic Evolution Over Time')
plt.show()

lda_model.get_document_topics(corpus[0])

# import matplotlib.pyplot as plt
# import seaborn as sns
# from scipy.cluster.hierarchy import linkage, dendrogram

# # Convert country_similarity dictionary to a matrix
# country_similarity_matrix = np.zeros((len(country_topic_distributions_avg), len(country_topic_distributions_avg)))
# for (idx1, country_region1), (idx2, country_region2) in zip(enumerate(country_topic_distributions_avg), enumerate(country_topic_distributions_avg)):
#     country_similarity_matrix[idx1, idx2] = country_similarity.get((country_region1, country_region2), 0)

# # Perform hierarchical clustering
# linkage_matrix = linkage(country_similarity_matrix, method='complete')

# # Plot dendrogram
# plt.figure(figsize=(10, 8))
# dendrogram(linkage_matrix, labels=list(country_topic_distributions_avg.keys()), leaf_rotation=90)
# plt.xlabel('Country/Region')
# plt.ylabel('Distance')
# plt.title('Dendrogram of Country/Region Similarity')
# plt.show()

from fuzzywuzzy import fuzz
from fuzzywuzzy import process

def assign_year_names(title_list, csv_file):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(csv_file)

    # Create a dictionary from the DataFrame with title as key and journal name as value
    title_year_dict = dict(zip(df['Title'], df['Publication Year']))

    # Assign journal names to the titles in the list using fuzzy matching
    assigned_year_names = []
    for title in title_list:
        match = process.extractOne(title, title_year_dict.keys(), scorer=fuzz.token_sort_ratio)
        if match[1] >= 14:  # Adjust the threshold as needed
            assigned_year_names.append(title_year_dict[match[0]])
        else:
            assigned_year_names.append("year not found")

    return assigned_year_names

# List of titles
title_list = titles
# CSV file containing title names and journal names
csv_file = "/content/part 2_last1.csv"

# Assign journal names to the list of titles
year_names = assign_year_names(title_list, csv_file)

print(year_names)

corpus_by_year = {}

for doc, year in zip(corpus, year_names):
    if year not in corpus_by_year:
        corpus_by_year[year] = []
    corpus_by_year[year].append(doc)

# Find years with NaN values
years_with_nan = []
for year, corpus in corpus_by_year.items():
    for doc in corpus:
        topics = lda_model.get_document_topics(doc)
        if any(np.isnan(prob) for _, prob in topics):
            years_with_nan.append(year)
            break

# Remove years with NaN values
cleaned_corpus_by_year = {year: corpus for year, corpus in corpus_by_year.items() if year not in years_with_nan}

x = len(cleaned_corpus_by_year)

from scipy.spatial.distance import jensenshannon
from scipy.special import kl_div
import numpy as np

# Initialize matrices to store pairwise divergences
jsd_matrix = np.zeros((x, x))
kl_matrix = np.zeros((x, x))

# Calculate divergences for each pair of subsets
for i, subset1 in enumerate(cleaned_corpus_by_year):
    for j, subset2 in enumerate(cleaned_corpus_by_year):
        # Get topic distributions for each subset
        topic_distributions1 = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in cleaned_corpus_by_year[subset1]]
        topic_distributions2 = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in cleaned_corpus_by_year[subset2]]

        # Convert topic distributions to probability vectors
        topic_probs1 = np.array([[topic_prob[1] for topic_prob in doc] for doc in topic_distributions1])
        topic_probs2 = np.array([[topic_prob[1] for topic_prob in doc] for doc in topic_distributions2])

        # Calculate average topic distributions for each subset
        avg_topic_probs1 = np.mean(topic_probs1, axis=0)
        avg_topic_probs2 = np.mean(topic_probs2, axis=0)

        # Calculate Jensen-Shannon divergence
        jsd_matrix[i, j] = np.mean([jensenshannon(p1, p2) for p1, p2 in zip(topic_probs1, topic_probs2)])

        # Calculate Kullback-Leibler divergence
        kl_matrix[i, j] = np.mean([np.sum(kl_div(p1, p2)) for p1, p2 in zip(topic_probs1, topic_probs2)])

# Print the matrices
print("Jensen-Shannon Divergence Matrix:")
print(len(jsd_matrix))
# print("\nKullback-Leibler Divergence Matrix:")
# print(kl_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

# Transpose the matrices
jsd_matrix_transposed = jsd_matrix.T
kl_matrix_transposed = kl_matrix.T
jsd_matrix_transposed[np.isnan(jsd_matrix_transposed)] = np.inf
jsd_matrix_transposed[np.isinf(jsd_matrix_transposed)] = np.nanmax(jsd_matrix_transposed)
jsd_matrix_transposed[np.isinf(jsd_matrix_transposed)] = 0



# Hierarchical clustering based on Jensen-Shannon Divergence
sns.clustermap(jsd_matrix_transposed, cmap="YlGnBu", figsize=(10, 10), method='average', metric='euclidean', row_cluster=True, col_cluster=True, dendrogram_ratio=(0.1, 0.2), xticklabels=corpus_by_year
               .keys(), yticklabels=range(jsd_matrix.shape[1]))
plt.title('Hierarchical Clustering Heatmap (Jensen-Shannon Divergence)')
plt.xlabel('Journals')
plt.ylabel('Topics')
plt.show()

# Hierarchical clustering based on Kullback-Leibler Divergence
# sns.clustermap(kl_matrix_transposed, cmap="YlGnBu", figsize=(10, 10), method='average', metric='euclidean', row_cluster=True, col_cluster=True, dendrogram_ratio=(0.1, 0.2), xticklabels=corpus_by_country.keys(), yticklabels=range(kl_matrix.shape[1]))
# plt.title('Hierarchical Clustering Heatmap (Kullback-Leibler Divergence)')
# plt.xlabel('Countries')
# plt.ylabel('Topics')
# plt.show()

len(cleaned_corpus_by_year)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Sample data
years = [2015, 2016, 2017, 2018, 2019]
topic1_proportions = [0.1, 0.2, 0.3, 0.4, 0.5]
topic2_proportions = [0.2, 0.3, 0.25, 0.2, 0.15]
topic3_proportions = [0.3, 0.25, 0.2, 0.15, 0.1]

# Create DataFrame
data = {
    'Year': years,
    'Topic 1': topic1_proportions,
    'Topic 2': topic2_proportions,
    'Topic 3': topic3_proportions
}
df = pd.DataFrame(data)
df.set_index('Year', inplace=True)

# Plot stacked area chart
plt.figure(figsize=(10, 6))
df.plot(kind='area', stacked=True)
plt.title('Topic Proportions Over Time')
plt.xlabel('Year')
plt.ylabel('Topic Proportion')
plt.legend(title='Topic')
plt.show()

# corpus_by_year.items()

import pandas as pd
import matplotlib.pyplot as plt

# Get topic proportions for each year
topic_proportions_by_year = {}

for year, year_corpus in cleaned_corpus_by_year.items():
    # Get topic proportions for the current year's corpus
    topic_proportions = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in year_corpus]

    # Convert topic proportions to DataFrame
    df_topic_proportions = pd.DataFrame([[topic_prob[1] for topic_prob in doc] for doc in topic_proportions])

    # Calculate mean topic proportions for the current year
    mean_topic_proportions = df_topic_proportions.mean()

    # Store mean topic proportions for the current year
    topic_proportions_by_year[year] = mean_topic_proportions

# Convert dictionary to DataFrame
df_topic_proportions_by_year = pd.DataFrame(topic_proportions_by_year).T
df_topic_proportions_by_year.columns = [f"Topic {i+1}" for i in range(len(df_topic_proportions_by_year.columns))]

# Plot stacked area chart for topic proportions over the years
plt.figure(figsize=(10, 6))
df_topic_proportions_by_year.plot(kind='area', stacked=True)
plt.title('Topic Proportions Over the Years')
plt.xlabel('Year')
plt.ylabel('Topic Proportion')
plt.legend(title='Topic')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Get topic proportions for each year
topic_proportions_by_year = {}

for year, year_corpus in cleaned_corpus_by_year.items():
    # Get topic proportions for the current year's corpus
    topic_proportions = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in year_corpus]

    # Convert topic proportions to DataFrame
    df_topic_proportions = pd.DataFrame([[topic_prob[1] for topic_prob in doc] for doc in topic_proportions])

    # Calculate mean topic proportions for the current year
    mean_topic_proportions = df_topic_proportions.mean()

    # Store mean topic proportions for the current year
    topic_proportions_by_year[year] = mean_topic_proportions

# Convert dictionary to DataFrame
df_topic_proportions_by_year = pd.DataFrame(topic_proportions_by_year).T
df_topic_proportions_by_year.columns = [f"Topic {i+1}" for i in range(len(df_topic_proportions_by_year.columns))]

# Plot stacked area chart for topic proportions over the years
plt.figure(figsize=(10, 6))
df_topic_proportions_by_year.plot(kind='area', stacked=True)
plt.title('Topic Proportions Over the Years')
plt.xlabel('Year')
plt.ylabel('Topic Proportion')
plt.legend(title='Topic')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Define the selected topics
selected_topics = ['Topic 1','Topic 2', 'Topic 3','Topic 4','Topic 5','Topic 6','Topic 7','Topic 8','Topic 9','Topic 10', 'Topic 11', 'Topic 12', 'Topic 13', 'Topic 14', 'Topic 15']

# Filter the DataFrame to include only selected topics
df_selected_topics = df_topic_proportions_by_year[selected_topics]

# Plot stacked area chart for selected topics over the years
plt.figure(figsize=(10, 6))
df_selected_topics.plot(kind='area', stacked=True)
plt.title('Topic Proportions Over the Years')
plt.xlabel('Year')
plt.ylabel('Topic Proportion')
plt.legend(title='Topic')
plt.show()

