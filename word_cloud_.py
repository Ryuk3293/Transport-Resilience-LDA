# -*- coding: utf-8 -*-
"""Word cloud .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wlc5gzSRTeESMKWnmlWmry4MWYayjXGj
"""

pip install pdfminer

pip install pyLDAvis

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
from pprint import pprint
# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
# spacy for lemmatization
import spacy
# Plotting tools
import pyLDAvis
import pyLDAvis.gensim_models  # don't skip this
import matplotlib.pyplot as plt
# %matplotlib inline
# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)
import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
# NLTK Stop words
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'slr', 'index', 'nile'])

# pip install wordcloud

import sys
# sys.setdefaultencoding("utf-8")
import imp
imp.reload(sys)
from pdfminer.pdfparser import PDFParser
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfpage import PDFTextExtractionNotAllowed
from pdfminer.pdfinterp import PDFResourceManager
from pdfminer.pdfinterp import PDFPageInterpreter
from pdfminer.pdfdevice import PDFDevice
from pdfminer.layout import LAParams, LTTextBox, LTTextLine, LTFigure, LTImage
from pdfminer.converter import PDFPageAggregator
import re
import os

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/Papers" #文件夹目录
files= os.listdir(path)
list_text = []
for file in files:
    if not os.path.isdir(file):
        fp = open(path+"/"+file,'rb')
        parser = PDFParser(fp)
        rsrcmgr = PDFResourceManager()
        laparams = LAParams()
        device = PDFPageAggregator(rsrcmgr, laparams=laparams)
        interpreter = PDFPageInterpreter(rsrcmgr, device)
        document = PDFDocument(parser)
        text_content = []
        for page in PDFPage.create_pages(document):
            interpreter.process_page(page)
            layout = device.get_result()
            for lt_obj in layout:
                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):
                    text_content.append(lt_obj.get_text())
                else:
                    pass
        total_text = ''.join(text_content).replace("\n"," ")
        list_text.append(total_text) # 这里可以不停加pdf进入列表
len(list_text)



def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))

data_words = list(sent_to_words(list_text))
# Build the bigram and trigram models
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)

# Faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

# # 删除停用词（stopword），建立二元模型和词形还原（Lemmatize）
# # Define functions for stopwords, bigrams, trigrams and lemmatization
# def remove_stopwords(texts):
#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

# def make_bigrams(texts):
#     return [bigram_mod[doc] for doc in texts]

# def make_trigrams(texts):
#     return [trigram_mod[bigram_mod[doc]] for doc in texts]

# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
#     """https://spacy.io/api/annotation"""
#     texts_out = []
#     for sent in texts:
#         doc = nlp(" ".join(sent))
#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
#     return texts_out

# # Remove Stop Words
# data_words_nostops = remove_stopwords(data_words)

# # Form Bigrams
# data_words_bigrams = make_bigrams(data_words_nostops)

# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# # python -m spacy download en_core_web_sm
# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

# # Do lemmatization keeping only noun, adj, vb, adv
# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

# print(data_lemmatized[:1])

import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Define a function to clean the text
def clean_text(text):
    # Remove non-alphanumeric characters and extra whitespaces
    cleaned_text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)

    # Convert text to lowercase
    cleaned_text = cleaned_text.lower()

    # Tokenize the text
    tokens = word_tokenize(cleaned_text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    # Get the set of English stopwords from NLTK


    # Define additional stopwords to add
    additional_stopwords = {'et', 'al', 'rtn', 'resilience','based','system', 'j', '0', '1', '2', 'c', '3','4', 'p', 'n', 'x'}

    # Add the additional stopwords to the set
    stop_words.update(additional_stopwords)
    tokens = [word for word in tokens if word not in stop_words]

    # Join tokens back into a single string
    cleaned_text = ' '.join(tokens)

    return cleaned_text

# Clean each text in the list
cleaned_texts = [clean_text(text) for text in list_text]

# # Print the cleaned text of the first PDF file
# print("Cleaned text of the first PDF file:")
# print(cleaned_texts[0][:500])  # Displaying the first 500 characters for illustration

# # Optionally, you may want to remove empty strings from the list
# cleaned_texts = [text for text in cleaned_texts if text.strip()]

# # Print the number of documents and the length of the cleaned text list
# print("\nNumber of PDF files processed:", len(list_text))
# print("Number of cleaned text documents:", len(cleaned_texts))

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Join all cleaned texts into a single string
all_text = ' '.join(cleaned_texts)

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()



# from wordcloud import WordCloud
# import matplotlib.pyplot as plt

# # Sample text
# text = data_lemmatized[0]
# # Generate word cloud
# wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# # Display the word cloud using matplotlib
# plt.figure(figsize=(10, 5))
# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis('off')
# plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Create TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.2, stop_words='english')

# Fit and transform the text data to TF-IDF vectors
tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_texts)

# Determine the optimal number of clusters using silhouette score
silhouette_scores = []
for k in range(2, 11):  # Trying cluster numbers from 2 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(tfidf_matrix)
    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Plot silhouette scores to find optimal number of clusters
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs Number of Clusters')
plt.show()

# Based on the plot, choose the optimal number of clusters
optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2
print("Optimal number of clusters:", optimal_num_clusters)

# Perform K-means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42)
kmeans.fit(tfidf_matrix)
clusters = kmeans.labels_

# Print the documents assigned to each cluster
for cluster_num in range(optimal_num_clusters):
    print("\nCluster", cluster_num + 1, ":")
    for i, text in enumerate(cleaned_texts):
        if clusters[i] == cluster_num:
            print("Document", i + 1)

# Note: This code assumes you have already cleaned and preprocessed your text data stored in 'cleaned_texts'.

# from collections import defaultdict
# import networkx as nx
# import matplotlib.pyplot as plt

# # Initialize a defaultdict to store word co-occurrences
# word_cooccurrences = defaultdict(int)

# # Iterate through each document in each cluster and update word co-occurrence counts
# for cluster_num in range(optimal_num_clusters):
#     cluster_texts = [cleaned_texts[i] for i, label in enumerate(clusters) if label == cluster_num]
#     for text in cluster_texts:
#         words = text.split()
#         for i, word1 in enumerate(words):
#             for j, word2 in enumerate(words):
#                 if i != j:
#                     word_cooccurrences[(word1, word2)] += 1

# # Create a graph to visualize word associations
# G = nx.Graph()

# # Add nodes for each word and edges for word co-occurrences
# for (word1, word2), cooccurrences in word_cooccurrences.items():
#     G.add_edge(word1, word2, weight=cooccurrences)

# # Visualize the graph
# plt.figure(figsize=(12, 8))
# pos = nx.spring_layout(G, k=0.1)  # Position nodes using the spring layout algorithm
# nx.draw_networkx_nodes(G, pos, node_size=100)
# nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)
# nx.draw_networkx_labels(G, pos, font_size=8)
# plt.title('Word Associations Network Graph')
# plt.axis('off')
# plt.show()

from collections import Counter

# Example text data
corpus = cleaned_texts

# Tokenize the text data into words
words = [word.lower() for sentence in corpus for word in sentence.split()]

# Count the frequency of each word
word_counts = Counter(words)

# Select the top 50 most common words
most_common_words = [word for word, _ in word_counts.most_common(15)]

# Keep only the most common 50 words in the corpus
corpus_filtered = [" ".join([word for word in sentence.split() if word.lower() in most_common_words]) for sentence in corpus]

# Print the filtered corpus
for sentence in corpus_filtered:
    print(sentence)

import networkx as nx

# Example text data
documents = corpus_filtered

# Preprocess text data (tokenization, lowercase, remove punctuation, etc.)
preprocessed_documents = [doc.lower().split() for doc in documents]

# Create co-occurrence matrix
co_occurrence_matrix = {}
for doc in preprocessed_documents:
    for i, word in enumerate(doc):
        if word not in co_occurrence_matrix:
            co_occurrence_matrix[word] = {}
        for j in range(max(0, i - 2), min(len(doc), i + 3)):
            if j != i:
                co_word = doc[j]
                co_occurrence_matrix[word][co_word] = co_occurrence_matrix[word].get(co_word, 0) + 1

# Create graph from co-occurrence matrix
G = nx.Graph()
for word, co_words in co_occurrence_matrix.items():
    for co_word, weight in co_words.items():
        G.add_edge(word, co_word, weight=weight)

# Visualize the network
nx.draw(G, with_labels=True)

